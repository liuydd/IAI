{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./1_word.txt', 'r', encoding='utf-8') as f:\n",
    "    unigram = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinyin_word = {pinyin: sum(item['counts']) for pinyin, item in unigram.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinyin_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2_word.txt', 'r', encoding='utf-8') as f:\n",
    "    bigram = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bigram:\n",
    "    print(i)\n",
    "    a = i.strip().split()\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2pinyin = {}\n",
    "with open('./word2pinyin.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, pinyin = line.strip().split()\n",
    "        word2pinyin[word] = pinyin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input():\n",
    "    while True:\n",
    "        try:\n",
    "            line = input().strip()\n",
    "            if not line:\n",
    "                break\n",
    "            pinyin_list = line.split()\n",
    "            # 在这里对拼音串进行处理\n",
    "            # 例如，输出它们或者进行其他操作\n",
    "            print(pinyin_list)\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成1_word.txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from pypinyin import lazy_pinyin, Style\n",
    "import time\n",
    "import sys\n",
    "\n",
    "start_time1 = time.time()\n",
    "#得到pinyin->汉字的转化, pinyin2char = {'pinyin': ['汉字1', '汉字2', ...], ...}\n",
    "with open('../training_data/拼音汉字表.txt', 'r', encoding='gbk') as f:\n",
    "    pinyin2char = {}\n",
    "    char2pinyin = {} #方便处理多音字\n",
    "    for line in f:\n",
    "        items = line.strip().split()\n",
    "        pinyin = items[0]\n",
    "        hanzi = items[1:]\n",
    "        if pinyin not in pinyin2char:\n",
    "            pinyin2char[pinyin] = []\n",
    "        pinyin2char[pinyin].extend(hanzi)\n",
    "        for word in items[1:]:\n",
    "            if word not in char2pinyin:\n",
    "                char2pinyin[word] = []\n",
    "            char2pinyin[word].append(pinyin)\n",
    "def is_multi_pinyin(c):\n",
    "    if c in char2pinyin and len(char2pinyin[c]) > 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "start_time2 = time.time()\n",
    "#有效的汉字，valid_char = [汉字1, 汉字2, ...]\n",
    "with open('../training_data/一二级汉字表.txt', 'r', encoding='gbk') as f:\n",
    "    valid_char = list(f.readline().strip())\n",
    "end_time2 = time.time()\n",
    "\n",
    "\n",
    "#处理语料库\n",
    "start_time3 = time.time()\n",
    "#得到单字的概率, char_possibility = {'pinyin': {'汉字1': 概率1, '汉字2': 概率2, ...}, ...}\n",
    "char_possibility = {}\n",
    "\n",
    "# 定义符号模式\n",
    "puncs = re.compile(r\"\\s|[a-zA-Z]|\\.|\\(|\\)|\" + \"|\".join([\"，\", \"。\", \"、\", \"：\", \"；\", \"？\", \"！\", \"（\", \"）\", \"《\", \"》\",\n",
    "                                                       \"-\", \"——\", \"·\", \"……\", \"‘\", \"’\", \"“\", \"”\", \"/\", r\"\\\\\", \"\\\\[\",\n",
    "                                                       \"\\\\]\", \"【\", \"】\", \"\\\\|\", \"℃\"]))\n",
    "\n",
    "def filter_sentence(sentence):\n",
    "    # 创建一个包含所有标点符号的字符串\n",
    "    punctuation = string.punctuation\n",
    "    # 过滤标点符号和数字\n",
    "    filtered_sentence = ''.join(char for char in sentence if char not in punctuation and not char.isdigit())\n",
    "    filtered_sentence = puncs.sub(\"\", filtered_sentence)\n",
    "    filtered_sentence = re.sub(r'[^\\u4e00-\\u9fa5]', '', filtered_sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "word_count = {}\n",
    "def cal_freq(file_name):\n",
    "    with open(file_name, 'r', encoding='gbk') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            html = data['html']\n",
    "            title = data['title']\n",
    "            article = title + html\n",
    "            article = filter_sentence(article)\n",
    "            for word in article:\n",
    "                #if word not in valid_char: continue\n",
    "                if word not in word_count:\n",
    "                    word_count[word] = 0\n",
    "                word_count[word] += 1 \n",
    "            with open('word_count.json', 'w', encoding = 'utf-8') as f:\n",
    "                json.dump(word_count, f, ensure_ascii=False, indent=4)\n",
    "             \n",
    "\n",
    "file_path = '../语料库/sina_news_gbk'\n",
    "files = os.listdir(file_path)\n",
    "for file in files[1:8]:\n",
    "    print(\"start\")\n",
    "    cal_freq(os.path.join(file_path, file))\n",
    "    print(\"finished this file\")\n",
    "\n",
    "import math\n",
    "for pinyin in pinyin2char:\n",
    "    item = {}\n",
    "    for hanzi in pinyin2char[pinyin]:\n",
    "        item[hanzi] = word_count.get(hanzi, 1)\n",
    "    char_possibility[pinyin] = item\n",
    "with open('char_frequency.json', 'w', encoding='utf-8') as f: \n",
    "    f.write(json.dumps(char_possibility, ensure_ascii=False, indent=2))\n",
    "for pinyin in char_possibility:\n",
    "    one_pinyin_count = sum(char_possibility[pinyin].values())\n",
    "    for hanzi in char_possibility[pinyin]:\n",
    "        char_possibility[pinyin][hanzi] = math.log(one_pinyin_count) - math.log(char_possibility[pinyin][hanzi])\n",
    "\n",
    "with open('char_possibility.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(char_possibility, f, ensure_ascii=False, indent=4)\n",
    "end_time3 = time.time()\n",
    "print('处理语料库得到单字概率耗时：', end_time3 - start_time3)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "with open('../training_data/拼音汉字表.txt', 'r', encoding='gbk') as f:\n",
    "    pinyin2char = {}\n",
    "    char2pinyin = {} #方便处理多音字\n",
    "    for line in f:\n",
    "        items = line.strip().split()\n",
    "        pinyin = items[0]\n",
    "        hanzi = items[1:]\n",
    "        if pinyin not in pinyin2char:\n",
    "            pinyin2char[pinyin] = []\n",
    "        pinyin2char[pinyin].extend(hanzi)\n",
    "        for word in items[1:]:\n",
    "            if word not in char2pinyin:\n",
    "                char2pinyin[word] = []\n",
    "            char2pinyin[word].append(pinyin)\n",
    "char_possibility = {}\n",
    "with open('word_count.json', 'r', encoding='utf-8') as f:\n",
    "    word_count = json.load(f)\n",
    "for pinyin in pinyin2char:\n",
    "    item = {}\n",
    "    for hanzi in pinyin2char[pinyin]:\n",
    "        item[hanzi] = word_count.get(hanzi, 1)\n",
    "    char_possibility[pinyin] = item\n",
    "with open('char_frequency.json', 'w', encoding='utf-8') as f: \n",
    "    f.write(json.dumps(char_possibility, ensure_ascii=False, indent=2))\n",
    "for pinyin in char_possibility:\n",
    "    one_pinyin_count = sum(char_possibility[pinyin].values())\n",
    "    for hanzi in char_possibility[pinyin]:\n",
    "        char_possibility[pinyin][hanzi] = math.log(one_pinyin_count) - math.log(char_possibility[pinyin][hanzi])\n",
    "\n",
    "with open('char_possibility.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(char_possibility, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinyin_list = {}\n",
    "with open('../training_data/拼音汉字表.txt', 'r', encoding='gbk') as f:\n",
    "    for line in f:\n",
    "        item = {}\n",
    "        words = line.strip().split()\n",
    "        item['words'] = words[1:]\n",
    "        item['counts'] = []\n",
    "        for word in words[1:]:\n",
    "            if word in word_count:\n",
    "                item['counts'].append(word_count[word])\n",
    "            else: item['counts'].append(0)\n",
    "        #item['counts'] = [word_count[word] for word in words[1:]]\n",
    "        pinyin_list[words[0]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./1_word.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(pinyin_list, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成2_word.txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import re\n",
    "import string\n",
    "# 定义符号模式\n",
    "puncs = re.compile(r\"\\s|[a-zA-Z]|\\.|\\(|\\)|\" + \"|\".join([\"，\", \"。\", \"、\", \"：\", \"；\", \"？\", \"！\", \"（\", \"）\", \"《\", \"》\",\n",
    "                                                       \"-\", \"——\", \"·\", \"……\", \"‘\", \"’\", \"“\", \"”\", \"/\", r\"\\\\\", \"\\\\[\",\n",
    "                                                       \"\\\\]\", \"【\", \"】\", \"\\\\|\", \"℃\"]))\n",
    "\n",
    "def filter_sentence(sentence):\n",
    "    # 创建一个包含所有标点符号的字符串\n",
    "    punctuation = string.punctuation\n",
    "    # 过滤标点符号和数字\n",
    "    filtered_sentence = ''.join(char for char in sentence if char not in punctuation and not char.isdigit())\n",
    "    filtered_sentence = puncs.sub(\"\", filtered_sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "import jieba\n",
    "\n",
    "def cut(article, two_word_count):\n",
    "    words = jieba.cut(article)\n",
    "    for i in range(len(article) - 1):\n",
    "        word = article[i:i+2]\n",
    "        if len(word) == 2:\n",
    "            two_word_count[word] = two_word_count.get(word, 0) + 1\n",
    "\n",
    "two_word_count = {}\n",
    "def cal_freq2(file_name):\n",
    "    with open(file_name, 'r', encoding='gbk') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            html = data['html']\n",
    "            title = data['title']\n",
    "            title = filter_sentence(title)\n",
    "            html = filter_sentence(html)\n",
    "            cut(title, two_word_count)\n",
    "            cut(html, two_word_count)\n",
    "                    \n",
    "import os\n",
    "file_path = '../语料库/sina_news_gbk'\n",
    "files = os.listdir(file_path)\n",
    "for file in files[1:8]:\n",
    "    cal_freq2(os.path.join(file_path, file))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import pinyin, Style\n",
    "result = {}\n",
    "for item in two_word_count:\n",
    "    gen = pinyin(item, style=Style.NORMAL)\n",
    "    if len(gen) < 2: continue\n",
    "    result[item] = ' '.join(gen[0] + gen[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "    # 统计词频\n",
    "for word, pin in result.items():\n",
    "    if pin not in output_dict:\n",
    "        output_dict[pin] = {'words': [], 'counts': []}\n",
    "        output_dict[pin]['words'].append(word[0] + ' ' + word[1])\n",
    "        output_dict[pin]['counts'].append(1)\n",
    "    else:\n",
    "        if word in output_dict[pin]['words']:\n",
    "            output_dict[pin]['counts'][output_dict[pin]['words'].index(word)] += 1\n",
    "        else:\n",
    "            output_dict[pin]['words'].append(word[0] + ' ' + word[1])\n",
    "            output_dict[pin]['counts'].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./2_word.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(output_dict, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试学姐的方式构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到pinyin->汉字的转化, pinyin2char = {'pinyin': ['汉字1', '汉字2', ...], ...}\n",
    "with open('./word2pinyin.txt', 'r', encoding='utf-8') as f:\n",
    "    pinyin2char = {}\n",
    "    for line in f:\n",
    "        word, pinyin = line.strip().split()\n",
    "        if pinyin not in pinyin2char:\n",
    "            pinyin2char[pinyin] = []\n",
    "        pinyin2char[pinyin].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到单字的概率, char_possibility = {'pinyin': {'汉字1': 概率1, '汉字2': 概率2, ...}, ...}\n",
    "import math\n",
    "import json\n",
    "with open('./1_word.txt', 'r', encoding='utf-8') as f:\n",
    "    unigram = json.load(f)    \n",
    "one_word_count = {}\n",
    "for pinyin in unigram:\n",
    "    if pinyin not in one_word_count:\n",
    "        one_word_count[pinyin] = 0\n",
    "    one_word_count[pinyin] += sum(unigram[pinyin]['counts'])\n",
    "char_possibility = {}\n",
    "for pinyin in unigram:\n",
    "    if pinyin not in char_possibility:\n",
    "        char_possibility[pinyin] = {}\n",
    "    cp = {}\n",
    "    for i in range(len(unigram[pinyin]['words'])):\n",
    "        word = unigram[pinyin]['words'][i]\n",
    "        count = unigram[pinyin]['counts'][i]\n",
    "        print(\"one_word_count: \", one_word_count)\n",
    "        print(\"word:\", word)\n",
    "        print(\"count: \", count)\n",
    "        poss = math.log(one_word_count[pinyin]) - math.log(count)\n",
    "        cp[word] = poss\n",
    "    char_possibility[pinyin] = cp\n",
    "    print(char_possibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_possibility = {'汉字': {'pinyin': {'汉字': 概率, ...}, ...}, ...}    \n",
    "with open('./2_word.txt', 'r', encoding='utf-8') as f:\n",
    "    bigram = json.load(f)\n",
    "two_word_count = {}\n",
    "for pinyins in bigram:\n",
    "    if pinyins not in two_word_count:\n",
    "        two_word_count[pinyins] = 0 \n",
    "    two_word_count[pinyins] += sum(bigram[pinyins]['counts'])\n",
    "word_possibility = {}\n",
    "for pinyin2 in bigram:\n",
    "    pinyin_list = pinyin2.strip().split()\n",
    "    word_list = bigram[pinyin2]['words']\n",
    "    count_list = bigram[pinyin2]['counts']\n",
    "    for i in range(len(word_list)):\n",
    "        words = word_list[i]\n",
    "        word = words.strip().split()\n",
    "        if word[0] not in word_possibility:\n",
    "            word_possibility[word[0]] = {}\n",
    "        if pinyin_list[1] not in word_possibility[word[0]]:\n",
    "            word_possibility[word[0]][pinyin_list[1]] = {}\n",
    "        word_possibility[word[0]][pinyin_list[1]][word[1]] = math.log(two_word_count[pinyin2]) - math.log(count_list[i])\n",
    "    print(word_possibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
